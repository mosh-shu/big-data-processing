# 大規模データ処理法 課題 1

**Shu Sakamoto**  
学籍番号: 71775236  
ログイン名: t17523ss

## 解析の目的

2019 年度春学期「大規模データ処理」の授業では, 2019 年 4 月における Wikipedia のアクセスログを解析した. このとき我々は 2019 年 4 月に人気のあったページを確認するため, 該当期間の総アクセス数の順位に着目した. しかし, この手法を用いると "トップページ" や "特別:検索" のように Wikipedia の性質上アクセスされやすいページが上位に表示されてしまい, ノイズとなってしまう. また, "白石麻衣" のように常に人気のあるページも表示されてしまうが, これは "4 月に人気のあるコンテンツ" としての側面が薄いページであるため, これもなるべく取り除きたい. つまり, 我々 j が着目したいのは任意の記事の 4 月におけるニュース性である. そこで本課題では, 慢性的に人気のあるページではなく, コンテンツのニュース性が高いページが上位に表示されるような解析を行なった.

## 解析の手順

ニュース性をはかる指標として, $\frac{\Delta A}{\Delta t}$ を用いる ($A$は任意の時間窓におけるアクセス数, $t$は時間). つまり, ある時間においてアクセス数がどれだけ増えたかに着目する. この指標を用いれば "トップページ" や "[]" といったページは上位に表示されず, 例えば "令和" のようなニュース性の高いページのみが上位に表示されるはずである.

`gzip -dc ../../../public/pv201904/pageviews-2019040[1-2]-000000.gz | ./mapper.py | sort | ./reducer.py 1000 | nkf -w --url-input | sort -k 2gr,2`
`hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar -files reducer.py,mapper.py -input pv201904 -output pv201904_diff-3 -mapper 'mapper.py ja' -reducer 'reducer.py 10000'`

## わかったこと

## やってみた感想
